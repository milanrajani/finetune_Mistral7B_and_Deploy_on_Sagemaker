{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Deploy open LLMs with Amazon SageMaker\n",
    "\n",
    "In this sagemaker example, we are going to learn how to fine-tune open LLMs, like [Llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf), [Falcon](https://huggingface.co/models?other=falcon) or [Mistral](https://huggingface.co/models?other=mistral) using [QLoRA](https://arxiv.org/abs/2305.14314) and how to deploy them afterwards using the [Hugging Face LLM Inference DLC](https://huggingface.co/blog/sagemaker-huggingface-llm)\n",
    "\n",
    "In our example, we are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). We will also make use of new and efficient features and methods including, Flash Attention, Datset Packing and Mixed Precision Training.\n",
    "\n",
    "In Detail you will learn how to:\n",
    "ðŸ›«\n",
    "\n",
    "## 1. Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.34.0\" \"datasets[s3]==2.13.0\" \"sagemaker>=2.190.0\" \"gradio==3.50.2\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Llama 2 you need to login into our hugging face account, to use your token for accessing the gated repository. We can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::136363710637:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-ap-south-1-136363710637\n",
      "sagemaker session region: ap-south-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "We will useÂ [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)Â an open source dataset of instruction-following records on categories outlined in theÂ [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load theÂ `dolly`Â dataset, we use theÂ `load_dataset()`Â method from the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bc1052fffa40dc96b6ab0dc0c52156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ffccd44f5f499d9e47bfb2177b7a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4f646d97fc4075bf7cd3b01d87bd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'Which of these sentences is correct: \"If I was you, I would do it\" or \"If I were you, I would do it\"?', 'context': '', 'response': 'The correct sentence is \"If I were you, I would do it\".\" This is the subjunctive form of English. It should be used when a hypothetical is contrary to fact. In this case, \"I\" is not \"you,\" so the subjunctive form should be used. In first person, the form of \"be\" used for the subjunctive mood is \"were.\"', 'category': 'open_qa'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "How are you doing today?\n",
      "\n",
      "### Answer\n",
      "As a Large Language Model (LLM), I don't have feelings like humans do. Thanks for asking though!\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training. This means that we are stacking multiple samples to one sequence and split them with an EOS Token. This makes the training more efficient. Packing/stacking samples can be done during training or before. We will do it before training to save time. We created a utility method [pack_dataset](./scripts/utils/pack_dataset.py) that takes a dataset and a packing function and returns a packed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cae7855a594e71b41d3d65ce1f3638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f2d63bd97d4e1c8309d0558248cd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb2867219224a379298745c1c8bb081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f91155b40b44b8b27fec1960231c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pack/stack our dataset we need to first tokenize it and then we can pack it with the `pack_dataset` method. To prepare our dataset we will now: \n",
    "1. Format our samples using the template method and add an EOS token at the end of each sample\n",
    "2. Tokenize our dataset to convert it from text to tokens\n",
    "3. Pack our dataset to 2048 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1b4b0ca9b5479eba22fe2fda4abde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Provide a bulleted list of things to think about when buying a house\n",
      "\n",
      "### Answer\n",
      "The following are things to think about when buying a house:\n",
      "1. Location\n",
      "2. Purchase price and mortgage rates\n",
      "3. Floor Plan / Layout\n",
      "4. Design / Finishings\n",
      "5. Garage space\n",
      "6. Storage space\n",
      "7. How long one plans to live there\n",
      "8. Property taxes\n",
      "9. Insurance rates\n",
      "10. Maintenance costs</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a386a9209d4fdd8a6c91f2f5bf3b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking dataset into chunks of 2048 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaaf830e6e954670a5d424e6fc3b08c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1528\n",
      "Total number of samples: 1528\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "# add utils method to path for loading dataset\n",
    "import sys\n",
    "sys.path.append(\"../scripts/utils\")\n",
    "from pack_dataset import pack_dataset\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# tokenize dataset\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "\n",
    "# chunk dataset\n",
    "lm_dataset = pack_dataset(dataset, chunk_length=2048) # We use 2048 as the maximum length for packing\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:275: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b7ea1895cd4ed48031941beb524bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-ap-south-1-136363710637/processed/mistral/dolly/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/mistral/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune Mistral 7B with QLoRA on Amazon SageMaker\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2305.14314)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a [run_qlora.py](./scripts/run_qlora.py), which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code. The model will be temporally offloaded to disk, if it is too large to fit into memory.\n",
    "\n",
    "In Addition to QLoRA we will leverage the new [Flash Attention 2 integrationg with Transformers](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flash-attention-2) to speed up the training. Flash Attention 2 is a new efficient attention mechanism that is up to 3x faster than the standard attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'num_train_epochs': 3,                            # number of training epochs\n",
    "  'per_device_train_batch_size': 6,                 # batch size for training\n",
    "  'gradient_accumulation_steps': 2,                 # Number of updates steps to accumulate\n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'bf16': True,                                     # use bfloat16 precision\n",
    "  'tf32': True,                                     # use tf32 precision\n",
    "  'learning_rate': 2e-4,                            # learning rate\n",
    "  'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\":\"constant\",                   # learning rate scheduler\n",
    "  'save_strategy': \"epoch\",                         # save strategy for checkpoints\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  'merge_adapters': True,                           # wether to merge LoRA into the model (needs more memory)\n",
    "  'use_flash_attn': True,                           # Whether to use Flash Attention\n",
    "  'output_dir': '/tmp/run',                         # output directory, where to save assets during training\n",
    "                                                    # could be used for checkpointing. The final trained\n",
    "                                                    # model will always be saved to s3 at the end of training\n",
    "}\n",
    "\n",
    "if HfFolder.get_token() is not None:\n",
    "    hyperparameters['hf_token'] = HfFolder.get_token() # huggingface token to access gated models, e.g. llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. Amazon SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n",
    "> Note: Make sure that you include the `requirements.txt` in the `source_dir` if you are using a custom training script. We recommend to just clone the whole repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{hyperparameters[\"model_id\"].replace(\"/\",\"-\").replace(\".\",\"-\")}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_qlora.py',    # train script\n",
    "    source_dir           = '../scripts',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True         # not compress output to save training time and cost\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use `merge_weights` parameter, since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them using [merge_adapter_weights.py](./scripts/merge_adapter_weights.py) after training.\n",
    "\n",
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-mistralai-Mistral-7B--2024-05-29-03-44-41-036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-29 03:44:41 Starting - Starting the training job...\n",
      "2024-05-29 03:44:58 Starting - Preparing the instances for training...\n",
      "2024-05-29 03:45:36 Downloading - Downloading the training image..................\n",
      "2024-05-29 03:48:37 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:22,456 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:22,473 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:22,482 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:22,484 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:23,847 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.34.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 121.5/121.5 kB 8.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.14.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.4.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.23.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.41.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.3 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0->-r requirements.txt (line 1)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.15,>=0.14 (from transformers==4.34.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.0->-r requirements.txt (line 2)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.0->-r requirements.txt (line 2)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.14.0->-r requirements.txt (line 2)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.0->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 3)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.0->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.0->-r requirements.txt (line 2)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0->-r requirements.txt (line 1)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 3)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 3)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7.7/7.7 MB 107.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.0-py3-none-any.whl (492 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 492.2/492.2 kB 52.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 72.9/72.9 kB 14.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.23.0-py3-none-any.whl (258 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 258.1/258.1 kB 39.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 92.6/92.6 MB 23.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.2/1.2 MB 83.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.8/3.8 MB 105.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 295.0/295.0 kB 42.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, safetensors, huggingface-hub, tokenizers, accelerate, transformers, datasets, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.23.0 bitsandbytes-0.41.1 datasets-2.14.0 huggingface-hub-0.17.3 peft-0.4.0 safetensors-0.4.3 tokenizers-0.14.1 transformers-4.34.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:39,531 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:39,531 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:39,566 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:39,593 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:39,620 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:39,630 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"gradient_accumulation_steps\": 2,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"merge_adapters\": true,\n",
      "        \"model_id\": \"mistralai/Mistral-7B-v0.1\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"output_dir\": \"/tmp/run\",\n",
      "        \"per_device_train_batch_size\": 6,\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"tf32\": true,\n",
      "        \"use_flash_attn\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-mistralai-Mistral-7B--2024-05-29-03-44-41-036\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-136363710637/huggingface-qlora-mistralai-Mistral-7B--2024-05-29-03-44-41-036/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qlora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"mistralai/Mistral-7B-v0.1\",\"num_train_epochs\":3,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":6,\"save_strategy\":\"epoch\",\"tf32\":true,\"use_flash_attn\":true,\"warmup_ratio\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qlora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qlora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-136363710637/huggingface-qlora-mistralai-Mistral-7B--2024-05-29-03-44-41-036/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"gradient_accumulation_steps\":2,\"gradient_checkpointing\":true,\"hf_token\":\"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\",\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"merge_adapters\":true,\"model_id\":\"mistralai/Mistral-7B-v0.1\",\"num_train_epochs\":3,\"output_dir\":\"/tmp/run\",\"per_device_train_batch_size\":6,\"save_strategy\":\"epoch\",\"tf32\":true,\"use_flash_attn\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-mistralai-Mistral-7B--2024-05-29-03-44-41-036\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-136363710637/huggingface-qlora-mistralai-Mistral-7B--2024-05-29-03-44-41-036/source/sourcedir.tar.gz\",\"module_name\":\"run_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qlora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--gradient_accumulation_steps\",\"2\",\"--gradient_checkpointing\",\"True\",\"--hf_token\",\"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\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--merge_adapters\",\"True\",\"--model_id\",\"mistralai/Mistral-7B-v0.1\",\"--num_train_epochs\",\"3\",\"--output_dir\",\"/tmp/run\",\"--per_device_train_batch_size\",\"6\",\"--save_strategy\",\"epoch\",\"--tf32\",\"True\",\"--use_flash_attn\",\"True\",\"--warmup_ratio\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=true\u001b[0m\n",
      "\u001b[34mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_ADAPTERS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=mistralai/Mistral-7B-v0.1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/tmp/run\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=6\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=epoch\u001b[0m\n",
      "\u001b[34mSM_HP_TF32=true\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FLASH_ATTN=true\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_qlora.py --bf16 True --dataset_path /opt/ml/input/data/training --gradient_accumulation_steps 2 --gradient_checkpointing True --hf_token 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 --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --merge_adapters True --model_id mistralai/Mistral-7B-v0.1 --num_train_epochs 3 --output_dir /tmp/run --per_device_train_batch_size 6 --save_strategy epoch --tf32 True --use_flash_attn True --warmup_ratio 0.03\u001b[0m\n",
      "\u001b[34m2024-05-29 03:49:39,660 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flash-attn in /opt/conda/lib/python3.10/site-packages (0.2.8)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.6/2.6 MB 59.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=121664358 sha256=fe2fb2ab1b2d949e24341db20bcc716504221b134a2c58efc21eae28ca1abcf1\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flash-attn\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[34mSuccessfully installed flash-attn-2.5.9.post1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mLogging into the Hugging Face Hub with token hf_RqbEgtL...\u001b[0m\n",
      "\u001b[34mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34mToken is valid (permission: write).\u001b[0m\n",
      "\u001b[34mYour token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34mLogin successful\u001b[0m\n",
      "\u001b[34mDownloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 571/571 [00:00<00:00, 5.30MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)fetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)fetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.1k/25.1k [00:00<00:00, 79.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   1%|          | 52.4M/9.94G [00:00<00:21, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   1%|          | 105M/9.94G [00:00<00:21, 456MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   2%|â–         | 157M/9.94G [00:00<00:23, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   2%|â–         | 199M/9.94G [00:00<00:23, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 252M/9.94G [00:00<00:22, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 304M/9.94G [00:00<00:22, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   4%|â–Ž         | 357M/9.94G [00:00<00:22, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   4%|â–         | 409M/9.94G [00:00<00:21, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   5%|â–         | 461M/9.94G [00:01<00:21, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   5%|â–Œ         | 514M/9.94G [00:01<00:21, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   6%|â–Œ         | 566M/9.94G [00:01<00:20, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   6%|â–Œ         | 619M/9.94G [00:01<00:20, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   7%|â–‹         | 671M/9.94G [00:01<00:20, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   7%|â–‹         | 724M/9.94G [00:01<00:20, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   8%|â–Š         | 776M/9.94G [00:01<00:20, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   8%|â–Š         | 828M/9.94G [00:01<00:20, 448MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   9%|â–‰         | 881M/9.94G [00:01<00:20, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   9%|â–‰         | 933M/9.94G [00:02<00:20, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  10%|â–‰         | 986M/9.94G [00:02<00:20, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  10%|â–ˆ         | 1.04G/9.94G [00:02<00:20, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  11%|â–ˆ         | 1.09G/9.94G [00:02<00:20, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  11%|â–ˆâ–        | 1.14G/9.94G [00:02<00:19, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 1.20G/9.94G [00:02<00:19, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 1.25G/9.94G [00:02<00:19, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  13%|â–ˆâ–Ž        | 1.30G/9.94G [00:02<00:19, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–Ž        | 1.35G/9.94G [00:03<00:19, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–        | 1.41G/9.94G [00:03<00:19, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–        | 1.46G/9.94G [00:03<00:19, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–Œ        | 1.51G/9.94G [00:03<00:19, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–Œ        | 1.56G/9.94G [00:03<00:19, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–Œ        | 1.61G/9.94G [00:03<00:19, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 1.67G/9.94G [00:03<00:18, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 1.72G/9.94G [00:03<00:18, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 1.77G/9.94G [00:04<00:18, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 1.82G/9.94G [00:04<00:18, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 1.88G/9.94G [00:04<00:18, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 1.93G/9.94G [00:04<00:18, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–‰        | 1.98G/9.94G [00:04<00:18, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  20%|â–ˆâ–ˆ        | 2.03G/9.94G [00:04<00:18, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆ        | 2.09G/9.94G [00:04<00:17, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 2.14G/9.94G [00:04<00:17, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 2.19G/9.94G [00:04<00:17, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 2.24G/9.94G [00:05<00:17, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 2.30G/9.94G [00:05<00:18, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–Ž       | 2.35G/9.94G [00:05<00:17, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–       | 2.40G/9.94G [00:05<00:17, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–       | 2.45G/9.94G [00:05<00:17, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 2.51G/9.94G [00:05<00:17, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–Œ       | 2.56G/9.94G [00:05<00:16, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–‹       | 2.61G/9.94G [00:05<00:16, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 2.66G/9.94G [00:06<00:16, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 2.72G/9.94G [00:06<00:16, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 2.77G/9.94G [00:06<00:16, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  28%|â–ˆâ–ˆâ–Š       | 2.82G/9.94G [00:06<00:16, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–‰       | 2.87G/9.94G [00:06<00:16, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–‰       | 2.93G/9.94G [00:06<00:16, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–‰       | 2.98G/9.94G [00:06<00:15, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–ˆ       | 3.03G/9.94G [00:06<00:15, 444MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 3.08G/9.94G [00:07<00:15, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 3.14G/9.94G [00:07<00:15, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 3.19G/9.94G [00:07<00:14, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.24G/9.94G [00:07<00:14, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.29G/9.94G [00:07<00:14, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 3.34G/9.94G [00:07<00:14, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 3.40G/9.94G [00:07<00:14, 461MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–      | 3.45G/9.94G [00:07<00:14, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.50G/9.94G [00:07<00:14, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.55G/9.94G [00:08<00:15, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 3.61G/9.94G [00:08<00:14, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3.66G/9.94G [00:08<00:14, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3.71G/9.94G [00:08<00:14, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.76G/9.94G [00:08<00:14, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.82G/9.94G [00:08<00:14, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 3.87G/9.94G [00:08<00:13, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 3.92G/9.94G [00:08<00:13, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3.97G/9.94G [00:09<00:13, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.03G/9.94G [00:09<00:13, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4.08G/9.94G [00:09<00:13, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.13G/9.94G [00:09<00:13, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.18G/9.94G [00:09<00:12, 448MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.24G/9.94G [00:09<00:12, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.29G/9.94G [00:09<00:12, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4.34G/9.94G [00:09<00:12, 448MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.39G/9.94G [00:09<00:12, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.45G/9.94G [00:10<00:18, 292MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.50G/9.94G [00:10<00:16, 326MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.55G/9.94G [00:10<00:15, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.60G/9.94G [00:10<00:14, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.66G/9.94G [00:10<00:13, 395MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.71G/9.94G [00:10<00:12, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.76G/9.94G [00:11<00:12, 412MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.81G/9.94G [00:11<00:12, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.87G/9.94G [00:11<00:12, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.92G/9.94G [00:11<00:11, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.97G/9.94G [00:11<00:11, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5.02G/9.94G [00:11<00:11, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5.06G/9.94G [00:11<00:11, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.11G/9.94G [00:11<00:11, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.15G/9.94G [00:11<00:11, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.19G/9.94G [00:12<00:11, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.23G/9.94G [00:12<00:11, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.27G/9.94G [00:12<00:11, 392MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5.32G/9.94G [00:12<00:11, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.36G/9.94G [00:12<00:11, 392MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.40G/9.94G [00:12<00:11, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.44G/9.94G [00:12<00:11, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.49G/9.94G [00:12<00:10, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.55G/9.94G [00:12<00:10, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.59G/9.94G [00:13<00:10, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.64G/9.94G [00:13<00:10, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.69G/9.94G [00:13<00:10, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.75G/9.94G [00:13<00:09, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.80G/9.94G [00:13<00:09, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.85G/9.94G [00:13<00:09, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.90G/9.94G [00:13<00:09, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.96G/9.94G [00:13<00:09, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6.01G/9.94G [00:14<00:09, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6.06G/9.94G [00:14<00:09, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.11G/9.94G [00:14<00:08, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.17G/9.94G [00:14<00:08, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.22G/9.94G [00:14<00:08, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.27G/9.94G [00:14<00:08, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6.32G/9.94G [00:14<00:08, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.38G/9.94G [00:14<00:08, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.43G/9.94G [00:15<00:08, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.48G/9.94G [00:15<00:08, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.53G/9.94G [00:15<00:07, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.59G/9.94G [00:15<00:07, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.64G/9.94G [00:15<00:07, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.69G/9.94G [00:15<00:07, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.74G/9.94G [00:15<00:07, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.79G/9.94G [00:15<00:07, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.85G/9.94G [00:15<00:07, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.90G/9.94G [00:16<00:06, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.95G/9.94G [00:16<00:06, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7.00G/9.94G [00:16<00:06, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7.06G/9.94G [00:16<00:06, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.11G/9.94G [00:16<00:06, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.16G/9.94G [00:16<00:06, 444MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.21G/9.94G [00:16<00:06, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.27G/9.94G [00:16<00:06, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 7.32G/9.94G [00:17<00:05, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.37G/9.94G [00:17<00:05, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.42G/9.94G [00:17<00:05, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.48G/9.94G [00:17<00:05, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.53G/9.94G [00:17<00:05, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.58G/9.94G [00:17<00:05, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.63G/9.94G [00:17<00:05, 444MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.69G/9.94G [00:17<00:05, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.74G/9.94G [00:17<00:04, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.79G/9.94G [00:18<00:04, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.84G/9.94G [00:18<00:04, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.90G/9.94G [00:18<00:04, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.95G/9.94G [00:18<00:05, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 7.99G/9.94G [00:18<00:05, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8.04G/9.94G [00:18<00:04, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.10G/9.94G [00:18<00:04, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.15G/9.94G [00:18<00:04, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.20G/9.94G [00:19<00:04, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8.25G/9.94G [00:19<00:03, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 8.30G/9.94G [00:19<00:03, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.36G/9.94G [00:19<00:03, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.41G/9.94G [00:19<00:03, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.46G/9.94G [00:19<00:03, 446MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.51G/9.94G [00:19<00:03, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.57G/9.94G [00:19<00:03, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.62G/9.94G [00:20<00:02, 444MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.67G/9.94G [00:20<00:02, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.72G/9.94G [00:20<00:02, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.78G/9.94G [00:20<00:02, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.83G/9.94G [00:20<00:02, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.88G/9.94G [00:20<00:02, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.93G/9.94G [00:20<00:02, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 8.99G/9.94G [00:20<00:02, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9.04G/9.94G [00:21<00:02, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.09G/9.94G [00:21<00:01, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.14G/9.94G [00:21<00:01, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.20G/9.94G [00:21<00:01, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9.25G/9.94G [00:21<00:01, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 9.30G/9.94G [00:21<00:01, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.35G/9.94G [00:21<00:01, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9.41G/9.94G [00:21<00:01, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.46G/9.94G [00:21<00:01, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.51G/9.94G [00:22<00:00, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.56G/9.94G [00:22<00:00, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.62G/9.94G [00:22<00:00, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.66G/9.94G [00:22<00:00, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.71G/9.94G [00:22<00:00, 414MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.76G/9.94G [00:22<00:00, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.81G/9.94G [00:22<00:00, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.87G/9.94G [00:22<00:00, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.92G/9.94G [00:23<00:00, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.94G/9.94G [00:23<00:00, 429MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:23<00:23, 23.41s/it]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   1%|          | 31.5M/4.54G [00:00<00:17, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   2%|â–         | 83.9M/4.54G [00:00<00:12, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   3%|â–Ž         | 126M/4.54G [00:00<00:12, 357MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   4%|â–         | 178M/4.54G [00:00<00:11, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   5%|â–         | 220M/4.54G [00:00<00:11, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   6%|â–Œ         | 262M/4.54G [00:00<00:11, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   7%|â–‹         | 315M/4.54G [00:00<00:10, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   8%|â–Š         | 357M/4.54G [00:00<00:10, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:   9%|â–‰         | 398M/4.54G [00:01<00:11, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  10%|â–‰         | 440M/4.54G [00:01<00:10, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  11%|â–ˆ         | 482M/4.54G [00:01<00:10, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 524M/4.54G [00:01<00:10, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  12%|â–ˆâ–        | 566M/4.54G [00:01<00:09, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  14%|â–ˆâ–Ž        | 619M/4.54G [00:01<00:09, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  15%|â–ˆâ–        | 671M/4.54G [00:01<00:09, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  16%|â–ˆâ–Œ        | 724M/4.54G [00:01<00:08, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  17%|â–ˆâ–‹        | 776M/4.54G [00:01<00:08, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  18%|â–ˆâ–Š        | 828M/4.54G [00:02<00:08, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  19%|â–ˆâ–‰        | 881M/4.54G [00:02<00:08, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  21%|â–ˆâ–ˆ        | 933M/4.54G [00:02<00:08, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  22%|â–ˆâ–ˆâ–       | 986M/4.54G [00:02<00:08, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 1.04G/4.54G [00:02<00:08, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  24%|â–ˆâ–ˆâ–       | 1.09G/4.54G [00:02<00:08, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 1.14G/4.54G [00:02<00:07, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  26%|â–ˆâ–ˆâ–‹       | 1.20G/4.54G [00:02<00:07, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  27%|â–ˆâ–ˆâ–‹       | 1.25G/4.54G [00:03<00:07, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  29%|â–ˆâ–ˆâ–Š       | 1.30G/4.54G [00:03<00:07, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  30%|â–ˆâ–ˆâ–‰       | 1.35G/4.54G [00:03<00:07, 412MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 1.39G/4.54G [00:03<00:07, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.44G/4.54G [00:03<00:07, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1.49G/4.54G [00:03<00:07, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1.53G/4.54G [00:03<00:08, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  35%|â–ˆâ–ˆâ–ˆâ–      | 1.57G/4.54G [00:03<00:08, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.61G/4.54G [00:04<00:08, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1.67G/4.54G [00:04<00:07, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.72G/4.54G [00:04<00:07, 392MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1.77G/4.54G [00:04<00:06, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1.81G/4.54G [00:04<00:06, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1.86G/4.54G [00:04<00:06, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.90G/4.54G [00:04<00:07, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1.94G/4.54G [00:04<00:06, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1.98G/4.54G [00:04<00:06, 385MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2.03G/4.54G [00:05<00:06, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2.08G/4.54G [00:05<00:06, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2.13G/4.54G [00:05<00:05, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2.18G/4.54G [00:05<00:05, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2.23G/4.54G [00:05<00:05, 414MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2.28G/4.54G [00:05<00:05, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2.32G/4.54G [00:05<00:05, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.37G/4.54G [00:05<00:05, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2.41G/4.54G [00:06<00:05, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.45G/4.54G [00:06<00:05, 396MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2.50G/4.54G [00:06<00:05, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2.54G/4.54G [00:06<00:05, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2.59G/4.54G [00:06<00:05, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.64G/4.54G [00:06<00:04, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.69G/4.54G [00:06<00:04, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.75G/4.54G [00:06<00:04, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.80G/4.54G [00:06<00:04, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2.85G/4.54G [00:07<00:04, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.90G/4.54G [00:07<00:04, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2.96G/4.54G [00:07<00:03, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3.01G/4.54G [00:07<00:03, 412MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3.06G/4.54G [00:07<00:03, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3.11G/4.54G [00:07<00:03, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3.17G/4.54G [00:07<00:03, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3.22G/4.54G [00:08<00:03, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.27G/4.54G [00:08<00:03, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3.32G/4.54G [00:08<00:02, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3.38G/4.54G [00:08<00:02, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3.43G/4.54G [00:08<00:02, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3.48G/4.54G [00:08<00:02, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3.53G/4.54G [00:08<00:02, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3.59G/4.54G [00:08<00:02, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3.63G/4.54G [00:08<00:02, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3.68G/4.54G [00:09<00:02, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3.72G/4.54G [00:09<00:02, 392MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3.77G/4.54G [00:09<00:01, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3.83G/4.54G [00:09<00:01, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3.88G/4.54G [00:09<00:01, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.93G/4.54G [00:09<00:01, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3.97G/4.54G [00:09<00:01, 392MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4.03G/4.54G [00:09<00:01, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4.07G/4.54G [00:10<00:01, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4.12G/4.54G [00:10<00:01, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.17G/4.54G [00:10<00:00, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4.23G/4.54G [00:10<00:00, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4.28G/4.54G [00:10<00:00, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4.33G/4.54G [00:10<00:00, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4.37G/4.54G [00:10<00:00, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4.42G/4.54G [00:10<00:00, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4.48G/4.54G [00:11<00:00, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4.53G/4.54G [00:11<00:00, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54G/4.54G [00:11<00:00, 405MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:34<00:00, 16.37s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:34<00:00, 17.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.32s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.45s/it]\u001b[0m\n",
      "\u001b[34mDownloading generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:00<00:00, 896kB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['down_proj', 'up_proj', 'q_proj', 'o_proj', 'v_proj', 'k_proj', 'gate_proj']\u001b[0m\n",
      "\u001b[34mtrainable params: 167,772,160 || all params: 3,919,843,328 || trainable%: 4.280073103982997\u001b[0m\n",
      "\u001b[34m0%|          | 0/381 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in float16.\u001b[0m\n",
      "\u001b[34mThe input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in float16.\u001b[0m\n",
      "\u001b[34m0%|          | 1/381 [00:35<3:41:40, 35.00s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/381 [01:09<3:40:29, 34.91s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/381 [01:44<3:39:44, 34.88s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/381 [02:19<3:39:04, 34.87s/it]\u001b[0m\n",
      "\u001b[34m1%|â–         | 5/381 [02:54<3:38:26, 34.86s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 6/381 [03:29<3:37:49, 34.85s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 7/381 [04:04<3:37:13, 34.85s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 8/381 [04:38<3:36:38, 34.85s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 9/381 [05:13<3:36:02, 34.85s/it]\u001b[0m\n",
      "\u001b[34m3%|â–Ž         | 10/381 [05:48<3:35:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5078, 'learning_rate': 0.0002, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m3%|â–Ž         | 10/381 [05:48<3:35:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m3%|â–Ž         | 11/381 [06:23<3:34:52, 34.84s/it]\u001b[0m\n",
      "\u001b[34m3%|â–Ž         | 12/381 [06:58<3:34:17, 34.84s/it]\u001b[0m\n",
      "\u001b[34m3%|â–Ž         | 13/381 [07:33<3:33:42, 34.84s/it]\u001b[0m\n",
      "\u001b[34m4%|â–Ž         | 14/381 [08:07<3:33:07, 34.84s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 15/381 [08:42<3:32:32, 34.84s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 16/381 [09:17<3:31:57, 34.84s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 17/381 [09:52<3:31:22, 34.84s/it]\u001b[0m\n",
      "\u001b[34m5%|â–         | 18/381 [10:27<3:30:47, 34.84s/it]\u001b[0m\n",
      "\u001b[34m5%|â–         | 19/381 [11:02<3:30:12, 34.84s/it]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 20/381 [11:37<3:29:37, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4579, 'learning_rate': 0.0002, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 20/381 [11:37<3:29:37, 34.84s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 21/381 [12:11<3:29:02, 34.84s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 22/381 [12:46<3:28:28, 34.84s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 23/381 [13:21<3:27:53, 34.84s/it]\u001b[0m\n",
      "\u001b[34m6%|â–‹         | 24/381 [13:56<3:27:18, 34.84s/it]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 25/381 [14:31<3:26:43, 34.84s/it]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 26/381 [15:06<3:26:08, 34.84s/it]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 27/381 [15:40<3:25:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 28/381 [16:15<3:24:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 29/381 [16:50<3:24:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 30/381 [17:25<3:23:48, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4374, 'learning_rate': 0.0002, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 30/381 [17:25<3:23:48, 34.84s/it]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 31/381 [18:00<3:23:14, 34.84s/it]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 32/381 [18:35<3:22:39, 34.84s/it]\u001b[0m\n",
      "\u001b[34m9%|â–Š         | 33/381 [19:09<3:22:04, 34.84s/it]\u001b[0m\n",
      "\u001b[34m9%|â–‰         | 34/381 [19:44<3:21:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m9%|â–‰         | 35/381 [20:19<3:20:55, 34.84s/it]\u001b[0m\n",
      "\u001b[34m9%|â–‰         | 36/381 [20:54<3:20:20, 34.84s/it]\u001b[0m\n",
      "\u001b[34m10%|â–‰         | 37/381 [21:29<3:19:45, 34.84s/it]\u001b[0m\n",
      "\u001b[34m10%|â–‰         | 38/381 [22:04<3:19:10, 34.84s/it]\u001b[0m\n",
      "\u001b[34m10%|â–ˆ         | 39/381 [22:38<3:18:35, 34.84s/it]\u001b[0m\n",
      "\u001b[34m10%|â–ˆ         | 40/381 [23:13<3:18:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4634, 'learning_rate': 0.0002, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m10%|â–ˆ         | 40/381 [23:13<3:18:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m11%|â–ˆ         | 41/381 [23:48<3:17:25, 34.84s/it]\u001b[0m\n",
      "\u001b[34m11%|â–ˆ         | 42/381 [24:23<3:16:51, 34.84s/it]\u001b[0m\n",
      "\u001b[34m11%|â–ˆâ–        | 43/381 [24:58<3:16:16, 34.84s/it]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 44/381 [25:33<3:15:41, 34.84s/it]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 45/381 [26:08<3:15:06, 34.84s/it]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 46/381 [26:42<3:14:31, 34.84s/it]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 47/381 [27:17<3:13:57, 34.84s/it]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–Ž        | 48/381 [27:52<3:13:22, 34.84s/it]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–Ž        | 49/381 [28:27<3:12:47, 34.84s/it]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–Ž        | 50/381 [29:02<3:12:12, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4312, 'learning_rate': 0.0002, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–Ž        | 50/381 [29:02<3:12:12, 34.84s/it]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–Ž        | 51/381 [29:37<3:11:37, 34.84s/it]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–Ž        | 52/381 [30:11<3:11:02, 34.84s/it]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 53/381 [30:46<3:10:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 54/381 [31:21<3:09:53, 34.84s/it]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 55/381 [31:56<3:09:17, 34.84s/it]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–        | 56/381 [32:31<3:08:43, 34.84s/it]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–        | 57/381 [33:06<3:08:08, 34.84s/it]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–Œ        | 58/381 [33:40<3:07:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–Œ        | 59/381 [34:15<3:06:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–Œ        | 60/381 [34:50<3:06:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4215, 'learning_rate': 0.0002, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–Œ        | 60/381 [34:50<3:06:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–Œ        | 61/381 [35:25<3:05:49, 34.84s/it]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–‹        | 62/381 [36:00<3:05:14, 34.84s/it]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 63/381 [36:35<3:04:39, 34.84s/it]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 64/381 [37:09<3:04:04, 34.84s/it]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 65/381 [37:44<3:03:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 66/381 [38:19<3:02:54, 34.84s/it]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 67/381 [38:54<3:02:19, 34.84s/it]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 68/381 [39:29<3:01:45, 34.84s/it]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 69/381 [40:04<3:01:10, 34.84s/it]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 70/381 [40:39<3:00:35, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4334, 'learning_rate': 0.0002, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 70/381 [40:39<3:00:35, 34.84s/it]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–Š        | 71/381 [41:13<3:00:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–‰        | 72/381 [41:48<2:59:25, 34.84s/it]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–‰        | 73/381 [42:23<2:58:51, 34.84s/it]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–‰        | 74/381 [42:58<2:58:16, 34.84s/it]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–‰        | 75/381 [43:33<2:57:41, 34.84s/it]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–‰        | 76/381 [44:08<2:57:06, 34.84s/it]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–ˆ        | 77/381 [44:42<2:56:31, 34.84s/it]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–ˆ        | 78/381 [45:17<2:55:57, 34.84s/it]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆ        | 79/381 [45:52<2:55:22, 34.84s/it]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆ        | 80/381 [46:27<2:54:47, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.426, 'learning_rate': 0.0002, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆ        | 80/381 [46:27<2:54:47, 34.84s/it]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆâ–       | 81/381 [47:02<2:54:12, 34.84s/it]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 82/381 [47:37<2:53:37, 34.84s/it]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 83/381 [48:11<2:53:02, 34.84s/it]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 84/381 [48:46<2:52:28, 34.84s/it]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 85/381 [49:21<2:51:53, 34.84s/it]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–Ž       | 86/381 [49:56<2:51:18, 34.84s/it]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–Ž       | 87/381 [50:31<2:50:43, 34.84s/it]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–Ž       | 88/381 [51:06<2:50:08, 34.84s/it]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–Ž       | 89/381 [51:41<2:49:34, 34.84s/it]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–Ž       | 90/381 [52:15<2:49:00, 34.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4491, 'learning_rate': 0.0002, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–Ž       | 90/381 [52:15<2:49:00, 34.85s/it]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 91/381 [52:50<2:48:25, 34.85s/it]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 92/381 [53:25<2:47:50, 34.85s/it]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 93/381 [54:00<2:47:15, 34.85s/it]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–       | 94/381 [54:35<2:46:40, 34.84s/it]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–       | 95/381 [55:10<2:46:05, 34.84s/it]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–Œ       | 96/381 [55:44<2:45:30, 34.84s/it]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–Œ       | 97/381 [56:19<2:44:55, 34.84s/it]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 98/381 [56:54<2:44:20, 34.84s/it]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 99/381 [57:29<2:43:45, 34.84s/it]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 100/381 [58:04<2:43:10, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4514, 'learning_rate': 0.0002, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 100/381 [58:04<2:43:10, 34.84s/it]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 101/381 [58:39<2:42:36, 34.84s/it]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 102/381 [59:14<2:42:01, 34.84s/it]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 103/381 [59:48<2:41:26, 34.84s/it]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 104/381 [1:00:23<2:40:51, 34.84s/it]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 105/381 [1:00:58<2:40:16, 34.84s/it]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 106/381 [1:01:33<2:39:41, 34.84s/it]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 107/381 [1:02:08<2:39:07, 34.84s/it]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 108/381 [1:02:43<2:38:32, 34.84s/it]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–Š       | 109/381 [1:03:17<2:37:57, 34.84s/it]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 110/381 [1:03:52<2:37:22, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4899, 'learning_rate': 0.0002, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 110/381 [1:03:52<2:37:22, 34.84s/it]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 111/381 [1:04:27<2:36:47, 34.84s/it]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 112/381 [1:05:02<2:36:12, 34.84s/it]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–‰       | 113/381 [1:05:37<2:35:37, 34.84s/it]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–‰       | 114/381 [1:06:12<2:35:03, 34.84s/it]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–ˆ       | 115/381 [1:06:46<2:34:28, 34.84s/it]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–ˆ       | 116/381 [1:07:21<2:33:53, 34.84s/it]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆ       | 117/381 [1:07:56<2:33:18, 34.84s/it]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆ       | 118/381 [1:08:31<2:32:43, 34.84s/it]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆ       | 119/381 [1:09:06<2:32:08, 34.84s/it]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆâ–      | 120/381 [1:09:41<2:31:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4793, 'learning_rate': 0.0002, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆâ–      | 120/381 [1:09:41<2:31:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 121/381 [1:10:16<2:30:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 122/381 [1:10:50<2:30:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 123/381 [1:11:25<2:29:49, 34.84s/it]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–Ž      | 124/381 [1:12:00<2:29:14, 34.84s/it]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–Ž      | 125/381 [1:12:35<2:28:39, 34.84s/it]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–Ž      | 126/381 [1:13:10<2:28:04, 34.84s/it]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–Ž      | 127/381 [1:13:45<2:27:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–Ž      | 128/381 [1:14:15<2:21:40, 33.60s/it]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 129/381 [1:14:50<2:22:40, 33.97s/it]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 130/381 [1:15:25<2:23:12, 34.23s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4024, 'learning_rate': 0.0002, 'epoch': 1.02}\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 130/381 [1:15:25<2:23:12, 34.23s/it]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 131/381 [1:16:00<2:23:24, 34.42s/it]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–      | 132/381 [1:16:35<2:23:21, 34.54s/it]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–      | 133/381 [1:17:09<2:23:09, 34.63s/it]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–Œ      | 134/381 [1:17:44<2:22:50, 34.70s/it]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–Œ      | 135/381 [1:18:19<2:22:26, 34.74s/it]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–Œ      | 136/381 [1:18:54<2:21:59, 34.77s/it]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–Œ      | 137/381 [1:19:29<2:21:29, 34.79s/it]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–Œ      | 138/381 [1:20:04<2:20:58, 34.81s/it]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–‹      | 139/381 [1:20:39<2:20:25, 34.82s/it]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 140/381 [1:21:13<2:19:52, 34.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2996, 'learning_rate': 0.0002, 'epoch': 1.1}\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 140/381 [1:21:13<2:19:52, 34.83s/it]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 141/381 [1:21:48<2:19:19, 34.83s/it]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 142/381 [1:22:23<2:18:45, 34.83s/it]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 143/381 [1:22:58<2:18:11, 34.84s/it]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 144/381 [1:23:33<2:17:36, 34.84s/it]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 145/381 [1:24:08<2:17:02, 34.84s/it]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 146/381 [1:24:42<2:16:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–Š      | 147/381 [1:25:17<2:15:52, 34.84s/it]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 148/381 [1:25:52<2:15:18, 34.84s/it]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 149/381 [1:26:27<2:14:43, 34.84s/it]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 150/381 [1:27:02<2:14:08, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3122, 'learning_rate': 0.0002, 'epoch': 1.18}\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 150/381 [1:27:02<2:14:08, 34.84s/it]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–‰      | 151/381 [1:27:37<2:13:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–‰      | 152/381 [1:28:12<2:12:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 153/381 [1:28:46<2:12:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 154/381 [1:29:21<2:11:49, 34.84s/it]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 155/381 [1:29:56<2:11:14, 34.84s/it]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 156/381 [1:30:31<2:10:39, 34.84s/it]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 157/381 [1:31:06<2:10:04, 34.84s/it]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/381 [1:31:41<2:09:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/381 [1:32:15<2:08:54, 34.84s/it]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/381 [1:32:50<2:08:19, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3346, 'learning_rate': 0.0002, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/381 [1:32:50<2:08:19, 34.84s/it]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/381 [1:33:25<2:07:45, 34.84s/it]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 162/381 [1:34:00<2:07:10, 34.84s/it]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 163/381 [1:34:35<2:06:35, 34.84s/it]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 164/381 [1:35:10<2:06:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 165/381 [1:35:44<2:05:25, 34.84s/it]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 166/381 [1:36:19<2:04:50, 34.84s/it]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 167/381 [1:36:54<2:04:15, 34.84s/it]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 168/381 [1:37:29<2:03:41, 34.84s/it]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 169/381 [1:38:04<2:03:06, 34.84s/it]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 170/381 [1:38:39<2:02:31, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3346, 'learning_rate': 0.0002, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 170/381 [1:38:39<2:02:31, 34.84s/it]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 171/381 [1:39:13<2:01:56, 34.84s/it]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 172/381 [1:39:48<2:01:21, 34.84s/it]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 173/381 [1:40:23<2:00:46, 34.84s/it]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 174/381 [1:40:58<2:00:12, 34.84s/it]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 175/381 [1:41:33<1:59:37, 34.84s/it]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 176/381 [1:42:08<1:59:02, 34.84s/it]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 177/381 [1:42:43<1:58:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 178/381 [1:43:17<1:57:52, 34.84s/it]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 179/381 [1:43:52<1:57:17, 34.84s/it]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 180/381 [1:44:27<1:56:43, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3477, 'learning_rate': 0.0002, 'epoch': 1.41}\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 180/381 [1:44:27<1:56:43, 34.84s/it]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 181/381 [1:45:02<1:56:08, 34.84s/it]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 182/381 [1:45:37<1:55:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 183/381 [1:46:12<1:54:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 184/381 [1:46:46<1:54:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 185/381 [1:47:21<1:53:49, 34.84s/it]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 186/381 [1:47:56<1:53:14, 34.84s/it]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 187/381 [1:48:31<1:52:39, 34.84s/it]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 188/381 [1:49:06<1:52:04, 34.84s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 189/381 [1:49:41<1:51:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 190/381 [1:50:15<1:50:54, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3415, 'learning_rate': 0.0002, 'epoch': 1.49}\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 190/381 [1:50:15<1:50:54, 34.84s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 191/381 [1:50:50<1:50:19, 34.84s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 192/381 [1:51:25<1:49:44, 34.84s/it]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 193/381 [1:52:00<1:49:09, 34.84s/it]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 194/381 [1:52:35<1:48:35, 34.84s/it]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 195/381 [1:53:10<1:48:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/381 [1:53:45<1:47:25, 34.84s/it]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/381 [1:54:19<1:46:50, 34.84s/it]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/381 [1:54:54<1:46:16, 34.84s/it]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 199/381 [1:55:29<1:45:41, 34.84s/it]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/381 [1:56:04<1:45:06, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3415, 'learning_rate': 0.0002, 'epoch': 1.57}\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/381 [1:56:04<1:45:06, 34.84s/it]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 201/381 [1:56:39<1:44:31, 34.84s/it]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 202/381 [1:57:14<1:43:56, 34.84s/it]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 203/381 [1:57:48<1:43:21, 34.84s/it]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 204/381 [1:58:23<1:42:46, 34.84s/it]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 205/381 [1:58:58<1:42:12, 34.84s/it]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 206/381 [1:59:33<1:41:37, 34.84s/it]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 207/381 [2:00:08<1:41:02, 34.84s/it]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 208/381 [2:00:43<1:40:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 209/381 [2:01:17<1:39:52, 34.84s/it]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 210/381 [2:01:52<1:39:17, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3302, 'learning_rate': 0.0002, 'epoch': 1.65}\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 210/381 [2:01:52<1:39:17, 34.84s/it]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 211/381 [2:02:27<1:38:43, 34.84s/it]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 212/381 [2:03:02<1:38:08, 34.84s/it]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 213/381 [2:03:37<1:37:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 214/381 [2:04:12<1:36:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 215/381 [2:04:47<1:36:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 216/381 [2:05:21<1:35:48, 34.84s/it]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 217/381 [2:05:56<1:35:13, 34.84s/it]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 218/381 [2:06:31<1:34:39, 34.84s/it]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 219/381 [2:07:06<1:34:04, 34.84s/it]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 220/381 [2:07:41<1:33:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3784, 'learning_rate': 0.0002, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 220/381 [2:07:41<1:33:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 221/381 [2:08:16<1:32:54, 34.84s/it]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 222/381 [2:08:50<1:32:19, 34.84s/it]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 223/381 [2:09:25<1:31:45, 34.84s/it]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 224/381 [2:10:00<1:31:10, 34.84s/it]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 225/381 [2:10:35<1:30:35, 34.84s/it]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 226/381 [2:11:10<1:30:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 227/381 [2:11:45<1:29:25, 34.84s/it]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 228/381 [2:12:19<1:28:50, 34.84s/it]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 229/381 [2:12:54<1:28:16, 34.84s/it]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 230/381 [2:13:29<1:27:41, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4062, 'learning_rate': 0.0002, 'epoch': 1.8}\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 230/381 [2:13:29<1:27:41, 34.84s/it]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 231/381 [2:14:04<1:27:06, 34.84s/it]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 232/381 [2:14:39<1:26:31, 34.84s/it]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 233/381 [2:15:14<1:25:56, 34.84s/it]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/381 [2:15:49<1:25:21, 34.84s/it]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/381 [2:16:23<1:24:46, 34.84s/it]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/381 [2:16:58<1:24:11, 34.84s/it]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 237/381 [2:17:33<1:23:37, 34.84s/it]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 238/381 [2:18:08<1:23:02, 34.84s/it]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 239/381 [2:18:43<1:22:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 240/381 [2:19:18<1:21:52, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3317, 'learning_rate': 0.0002, 'epoch': 1.88}\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 240/381 [2:19:18<1:21:52, 34.84s/it]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 241/381 [2:19:52<1:21:17, 34.84s/it]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 242/381 [2:20:27<1:20:42, 34.84s/it]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 243/381 [2:21:02<1:20:08, 34.84s/it]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 244/381 [2:21:37<1:19:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 245/381 [2:22:12<1:18:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 246/381 [2:22:47<1:18:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 247/381 [2:23:21<1:17:48, 34.84s/it]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 248/381 [2:23:56<1:17:13, 34.84s/it]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 249/381 [2:24:31<1:16:38, 34.84s/it]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 250/381 [2:25:06<1:16:04, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3331, 'learning_rate': 0.0002, 'epoch': 1.96}\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 250/381 [2:25:06<1:16:04, 34.84s/it]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 251/381 [2:25:41<1:15:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 252/381 [2:26:16<1:14:54, 34.84s/it]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 253/381 [2:26:50<1:14:19, 34.84s/it]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 254/381 [2:27:25<1:13:44, 34.84s/it]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 255/381 [2:27:55<1:09:37, 33.16s/it]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 256/381 [2:28:31<1:11:02, 34.10s/it]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 257/381 [2:29:06<1:10:56, 34.32s/it]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 258/381 [2:29:41<1:10:41, 34.48s/it]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 259/381 [2:30:15<1:10:19, 34.59s/it]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 260/381 [2:30:50<1:09:54, 34.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2849, 'learning_rate': 0.0002, 'epoch': 2.04}\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 260/381 [2:30:50<1:09:54, 34.67s/it]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 261/381 [2:31:25<1:09:26, 34.72s/it]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 262/381 [2:32:00<1:08:55, 34.75s/it]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 263/381 [2:32:35<1:08:24, 34.78s/it]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 264/381 [2:33:10<1:07:51, 34.80s/it]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 265/381 [2:33:44<1:07:18, 34.81s/it]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 266/381 [2:34:19<1:06:44, 34.82s/it]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 267/381 [2:34:54<1:06:10, 34.83s/it]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 268/381 [2:35:29<1:05:36, 34.83s/it]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 269/381 [2:36:04<1:05:01, 34.84s/it]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 270/381 [2:36:39<1:04:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2337, 'learning_rate': 0.0002, 'epoch': 2.12}\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 270/381 [2:36:39<1:04:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 271/381 [2:37:14<1:03:52, 34.84s/it]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 272/381 [2:37:48<1:03:17, 34.84s/it]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 273/381 [2:38:23<1:02:42, 34.84s/it]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 274/381 [2:38:58<1:02:08, 34.84s/it]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 275/381 [2:39:33<1:01:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 276/381 [2:40:08<1:00:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 277/381 [2:40:43<1:00:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 278/381 [2:41:17<59:48, 34.84s/it]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 279/381 [2:41:52<59:13, 34.84s/it]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 280/381 [2:42:27<58:39, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1612, 'learning_rate': 0.0002, 'epoch': 2.2}\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 280/381 [2:42:27<58:39, 34.84s/it]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 281/381 [2:43:02<58:04, 34.84s/it]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 282/381 [2:43:37<57:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 283/381 [2:44:12<56:54, 34.84s/it]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 284/381 [2:44:46<56:19, 34.84s/it]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 285/381 [2:45:21<55:44, 34.84s/it]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 286/381 [2:45:56<55:10, 34.84s/it]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 287/381 [2:46:31<54:35, 34.84s/it]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 288/381 [2:47:06<54:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 289/381 [2:47:41<53:25, 34.84s/it]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 290/381 [2:48:16<52:50, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1658, 'learning_rate': 0.0002, 'epoch': 2.27}\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 290/381 [2:48:16<52:50, 34.84s/it]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 291/381 [2:48:50<52:15, 34.84s/it]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 292/381 [2:49:25<51:40, 34.84s/it]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 293/381 [2:50:00<51:06, 34.84s/it]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 294/381 [2:50:35<50:31, 34.84s/it]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 295/381 [2:51:10<49:56, 34.84s/it]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 296/381 [2:51:45<49:21, 34.84s/it]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 297/381 [2:52:19<48:46, 34.84s/it]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 298/381 [2:52:54<48:11, 34.84s/it]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 299/381 [2:53:29<47:37, 34.84s/it]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 300/381 [2:54:04<47:02, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2079, 'learning_rate': 0.0002, 'epoch': 2.35}\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 300/381 [2:54:04<47:02, 34.84s/it]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 301/381 [2:54:39<46:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 302/381 [2:55:14<45:52, 34.84s/it]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 303/381 [2:55:48<45:17, 34.84s/it]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 304/381 [2:56:23<44:42, 34.84s/it]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 305/381 [2:56:58<44:07, 34.84s/it]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 306/381 [2:57:33<43:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 307/381 [2:58:08<42:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 308/381 [2:58:43<42:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 309/381 [2:59:17<41:48, 34.84s/it]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/381 [2:59:52<41:13, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2608, 'learning_rate': 0.0002, 'epoch': 2.43}\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 310/381 [2:59:52<41:13, 34.84s/it]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 311/381 [3:00:27<40:38, 34.84s/it]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 312/381 [3:01:02<40:04, 34.84s/it]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 313/381 [3:01:37<39:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 314/381 [3:02:12<38:54, 34.84s/it]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 315/381 [3:02:47<38:19, 34.84s/it]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 316/381 [3:03:21<37:44, 34.84s/it]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 317/381 [3:03:56<37:09, 34.84s/it]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 318/381 [3:04:31<36:34, 34.84s/it]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 319/381 [3:05:06<36:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 320/381 [3:05:41<35:25, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2113, 'learning_rate': 0.0002, 'epoch': 2.51}\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 320/381 [3:05:41<35:25, 34.84s/it]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 321/381 [3:06:16<34:50, 34.84s/it]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 322/381 [3:06:50<34:15, 34.84s/it]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 323/381 [3:07:25<33:40, 34.84s/it]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 324/381 [3:08:00<33:05, 34.84s/it]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 325/381 [3:08:35<32:31, 34.84s/it]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 326/381 [3:09:10<31:56, 34.84s/it]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 327/381 [3:09:45<31:21, 34.84s/it]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 328/381 [3:10:19<30:46, 34.84s/it]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 329/381 [3:10:54<30:11, 34.84s/it]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 330/381 [3:11:29<29:36, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1962, 'learning_rate': 0.0002, 'epoch': 2.59}\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 330/381 [3:11:29<29:36, 34.84s/it]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 331/381 [3:12:04<29:02, 34.84s/it]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 332/381 [3:12:39<28:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 333/381 [3:13:14<27:52, 34.84s/it]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 334/381 [3:13:49<27:17, 34.84s/it]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 335/381 [3:14:23<26:42, 34.84s/it]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 336/381 [3:14:58<26:07, 34.84s/it]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 337/381 [3:15:33<25:33, 34.84s/it]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 338/381 [3:16:08<24:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 339/381 [3:16:43<24:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 340/381 [3:17:18<23:48, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1624, 'learning_rate': 0.0002, 'epoch': 2.67}\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 340/381 [3:17:18<23:48, 34.84s/it]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 341/381 [3:17:52<23:13, 34.84s/it]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 342/381 [3:18:27<22:38, 34.84s/it]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 343/381 [3:19:02<22:03, 34.84s/it]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 344/381 [3:19:37<21:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 345/381 [3:20:12<20:54, 34.84s/it]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 346/381 [3:20:47<20:19, 34.84s/it]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 347/381 [3:21:21<19:44, 34.84s/it]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 348/381 [3:21:56<19:09, 34.84s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 349/381 [3:22:31<18:34, 34.84s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 350/381 [3:23:06<18:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1835, 'learning_rate': 0.0002, 'epoch': 2.75}\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 350/381 [3:23:06<18:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 351/381 [3:23:41<17:25, 34.84s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 352/381 [3:24:16<16:50, 34.84s/it]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 353/381 [3:24:50<16:15, 34.84s/it]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 354/381 [3:25:25<15:40, 34.84s/it]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 355/381 [3:26:00<15:05, 34.84s/it]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 356/381 [3:26:35<14:31, 34.84s/it]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 357/381 [3:27:10<13:56, 34.84s/it]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 358/381 [3:27:45<13:21, 34.84s/it]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 359/381 [3:28:20<12:46, 34.84s/it]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 360/381 [3:28:54<12:11, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2192, 'learning_rate': 0.0002, 'epoch': 2.82}\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 360/381 [3:28:54<12:11, 34.84s/it]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 361/381 [3:29:29<11:36, 34.84s/it]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 362/381 [3:30:04<11:01, 34.84s/it]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 363/381 [3:30:39<10:27, 34.84s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 364/381 [3:31:14<09:52, 34.84s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 365/381 [3:31:49<09:17, 34.84s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 366/381 [3:32:23<08:42, 34.84s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 367/381 [3:32:58<08:07, 34.84s/it]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 368/381 [3:33:33<07:32, 34.84s/it]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 369/381 [3:34:08<06:58, 34.84s/it]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 370/381 [3:34:43<06:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2378, 'learning_rate': 0.0002, 'epoch': 2.9}\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 370/381 [3:34:43<06:23, 34.84s/it]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 371/381 [3:35:18<05:48, 34.84s/it]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 372/381 [3:35:52<05:13, 34.84s/it]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 373/381 [3:36:27<04:38, 34.84s/it]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 374/381 [3:37:02<04:03, 34.84s/it]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 375/381 [3:37:37<03:29, 34.84s/it]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 376/381 [3:38:12<02:54, 34.84s/it]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 377/381 [3:38:47<02:19, 34.84s/it]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 378/381 [3:39:22<01:44, 34.84s/it]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 379/381 [3:39:56<01:09, 34.84s/it]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 380/381 [3:40:31<00:34, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1909, 'learning_rate': 0.0002, 'epoch': 2.98}\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 380/381 [3:40:31<00:34, 34.84s/it]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [3:41:06<00:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 13268.0114, 'train_samples_per_second': 0.345, 'train_steps_per_second': 0.029, 'train_loss': 1.332675732339774, 'epoch': 2.99}\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [3:41:08<00:00, 34.84s/it]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [3:41:08<00:00, 34.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.60s/it]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 967/967 [00:00<00:00, 8.35MB/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 493k/493k [00:00<00:00, 90.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.80M/1.80M [00:00<00:00, 1.88MB/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.80M/1.80M [00:00<00:00, 1.88MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72.0/72.0 [00:00<00:00, 594kB/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m2024-05-29 07:36:16,898 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-29 07:36:16,898 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-29 07:36:16,898 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-05-29 07:37:20 Uploading - Uploading generated training model\n",
      "2024-05-29 07:38:02 Completed - Training job completed\n",
      "Training seconds: 13961\n",
      "Billable seconds: 13961\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example for Mistral 7B, the SageMaker training job took `13968 seconds`, which is about `3.9 hours`. The ml.g5.4xlarge instance we used costs `$2.03 per hour` for on-demand usage. As a result, the total cost for training our fine-tuned Mistral model was only ~`$8`. \n",
    "\n",
    "Now lets make sure SageMaker has successfully uploaded the model to S3. We can use the `model_data` property of the estimator to get the S3 path to the model. Since we used `merge_weights=True` and `disable_output_compression=True` the model is stored as raw files in the S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.console.aws.amazon.com/s3/buckets/sagemaker-ap-south-1-136363710637/huggingface-qlora-mistralai-Mistral-7B--2024-05-29-03-44-41-036/output/model/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"].replace(\"s3://\", \"https://s3.console.aws.amazon.com/s3/buckets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a similar folder structure and files in your S3 bucket:\n",
    "\n",
    "![S3 Bucket](../assets/s3.png)\n",
    "\n",
    "Now, lets deploy our model to an endpoint. ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Fine-tuned Mistral 7B on Amazon SageMaker\n",
    "\n",
    "We are going to use the [Hugging Face LLM Inference DLC](https://huggingface.co/blog/sagemaker-huggingface-llm#what-is-hugging-face-llm-inference-dlc) a purpose-built Inference Container to easily deploy LLMs in a secure and managed environment. The DLC is powered by [Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/index) solution for deploying and serving Large Language Models (LLMs).\n",
    "\n",
    "Compared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to ourÂ `HuggingFaceModel`Â model class with aÂ `image_uri`Â pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use theÂ `get_huggingface_llm_image_uri`Â method provided by theÂ `sagemaker`Â SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specifiedÂ `backend`,Â `session`,Â `region`, andÂ `version`. You can find the available versionsÂ [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py39\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.ap-south-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.1.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a `HuggingFaceModel` using the container uri and the S3 path to our model. We also need to set our TGI configuration including the number of GPUs, max input tokens. You can find a full list of configuration options [here](https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the HuggingFaceModel we can deploy it to Amazon SageMaker using the deploy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2024-05-29-07-52-26-716\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2024-05-29-07-52-27-216\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2024-05-29-07-52-27-216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "\n",
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deep learning is a branch of machine learning based on a group of connected nodes that loosely imitate neurons in the brain. They are arranged in layers to make a network. These networks have been able to learn and even mimic many things humans do by observing data, similar to how babies observe their environment to learn about the world and interact with it.\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(message, history, system_prompt):\n",
    "    prompt = \"\"\n",
    "    if system_prompt:\n",
    "        prompt += f\"System: {system_prompt}\\n\"\n",
    "    for user_prompt, bot_response in history:\n",
    "        prompt += f\"User: {user_prompt}\\n\"\n",
    "        prompt += f\"Falcon: {bot_response}\\n\"  # Response already contains \"Falcon: \"\n",
    "    prompt += f\"\"\"User: {message}\n",
    "Falcon:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"\\nUser:\", \"<|endoftext|>\", \" User:\", \"###\"],\n",
    "}\n",
    "\n",
    "formatted_prompt = format_prompt(\"What is deep learning?\", [], \"You are a helpful assistant.\")\n",
    "payload = {\"inputs\": formatted_prompt, \"parameters\": parameters}\n",
    "chat = llm.predict(payload)\n",
    "print(chat[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deep learning is a subfield of machine learning based on the neural networks that work well with big data and can learn to perform specific tasks by themselves.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "custom_attributes = \"c000b4f9-df62-4c85-a0bf-7c525f9104a4\"\n",
    "endpoint_name = \"huggingface-pytorch-tgi-inference-2024-05-29-07-52-27-216\"\n",
    "content_type = \"application/json\"\n",
    "payload = {\"inputs\": formatted_prompt, \"parameters\": parameters}\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType=content_type,\n",
    "                                  Body=json.dumps(payload))\n",
    "\n",
    "generation = json.loads(response['Body'].read().decode('utf-8'))\n",
    "final_response = generation[0]['generated_text']\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stream Inference Requests from the Deployed Model\n",
    "\n",
    "[Amazon SageMaker supports streaming responses](https://aws.amazon.com/de/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/) from your model. We can use this to stream responses, we can leverage this to create a streaming gradio application with a better user experience.\n",
    "\n",
    "We created a sample application that you can use to test your model. You can find the code in [gradio-app.py](../demo/sagemaker_chat.py). The application will stream the responses from the model and display them in the UI. You can also use the application to test your model with your own inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 3.50.2, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://39577398e7546e72e2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://39577398e7546e72e2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add apps directory to path ../apps/\n",
    "import sys\n",
    "sys.path.append(\"../demo\")\n",
    "from sagemaker_chat import create_gradio_app\n",
    "\n",
    "# hyperparameters for llm\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"###\", \"</s>\"],\n",
    "}\n",
    "\n",
    "# define format function for our input\n",
    "def format_prompt(message, history, system_prompt):\n",
    "    prompt = \"\"\n",
    "    for user_prompt, bot_response in history:\n",
    "        prompt += f\"### Instruction\\n{user_prompt}\\n\\n\"\n",
    "        prompt += f\"### Answer\\n{bot_response}\\n\\n\"  # Response already contains \"Falcon: \"\n",
    "    prompt += f\"### Instruction\\n{message}\\n\\n### Answer\\n\"\n",
    "    return prompt\n",
    "\n",
    "# create gradio app\n",
    "create_gradio_app(\n",
    "    llm.endpoint_name,           # Sagemaker endpoint name\n",
    "    session=sess.boto_session,   # boto3 session used to send request\n",
    "    parameters=parameters,       # Request parameters\n",
    "    system_prompt=None,          # System prompt to use -> Mistral does not support system prompts\n",
    "    format_prompt=format_prompt, # Function to format prompt\n",
    "    concurrency_count=4,         # Number of concurrent requests\n",
    "    share=True,                  # Share app publicly\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gradio](../assets/gradio.png)\n",
    "\n",
    "Don't forget to delete the endpoint after you are done with the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
